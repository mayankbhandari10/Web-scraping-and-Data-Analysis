{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GithubTopRepoScraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCxeFsew9Aeo6OyyMtPFGA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayankbhandari10/Web-scraping-using-python/blob/main/GithubTopRepoScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''Scraping Top Repositories for Topics on GitHub\n",
        "Introduction about web scraping\n",
        "Introduction about GitHub and the problem statement\n",
        "Mention the tools you're using (Python, requests, Beautiful Soup, Pandas)\n",
        "\n",
        "\n",
        "Here are the steps we'll follow:\n",
        "\n",
        "We're going to scrape https://github.com/topics\n",
        "We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
        "For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
        "For each repository, we'll grab the repo name, username, stars and repo URL\n",
        "For each topic we'll create a CSV file in the following format:\n",
        "Repo Name,Username,Stars,Repo URL\n",
        "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
        "libgdx,libgdx,18300,https://github.com/libgdx/libgdx \n",
        "\n",
        "\n",
        "\n",
        "Scrape the list of topics from Github\n",
        "Explain how you'll do it.\n",
        "\n",
        "use requests to downlaod the page\n",
        "user BS4 to parse and extract information\n",
        "convert to a Pandas dataframe\n",
        "Let's write a function to download the page.'''"
      ],
      "metadata": {
        "id": "xruCw_BGibN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests --upgrade --quiet\n",
        "!pip install beautifulsoup4 --upgrade --quiet\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "RisEVvtKetMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748d1fc5-2e46-4f9e-bc66-722fbd915ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 20 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 30 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 40 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 51 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 61 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 62 kB 866 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 97 kB 4.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "LAbcD_AZfQ_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below function is for opening a web page "
      ],
      "metadata": {
        "id": "9Wbz45_QbEAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fn_open_webpage():\n",
        "      topic_url='https://github.com/topics'\n",
        "      response=requests.get(topic_url)\n",
        "      if response.status_code !=200:\n",
        "        raise Exception('failed to load page{}'.format(topic_url))\n",
        "      page_contents=response.text\n",
        "      soup = BeautifulSoup(page_contents, 'html.parser')\n",
        "      return page_contents \n",
        "      #return page_contents\n"
      ],
      "metadata": {
        "id": "tpR-9HiOeGUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_contents=fn_open_webpage()"
      ],
      "metadata": {
        "id": "FiP8G_xMe-Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fn_extract_the_topic() function will extract the topic from the github page"
      ],
      "metadata": {
        "id": "PPlvo3tubMMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fn_extract_the_topic():\n",
        "  \n",
        "  try:\n",
        "    topic_titles=[]\n",
        "    #page_contents=response.text\n",
        "    soup = BeautifulSoup(page_contents, 'html.parser')  \n",
        "    selection_class='f3 lh-condensed mb-0 mt-1 Link--primary'\n",
        "    topic_title=soup.find_all('p',{'class': selection_class})\n",
        "    for tag in topic_title:\n",
        "      topic_titles.append(tag.text)\n",
        "      len(topic_titles)\n",
        "  except:\n",
        "    raise Exception('failed to extract the topics')\n",
        "  return topic_titles\n",
        "  print(topic_titles)\n"
      ],
      "metadata": {
        "id": "bgUpvCBgi0Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skill_top=fn_extract_the_topic()"
      ],
      "metadata": {
        "id": "M89rGkQuj0F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fn_extract_the_description() will extract the description of skills"
      ],
      "metadata": {
        "id": "y3QpN8FYbXgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fn_extract_the_description():\n",
        "  try:\n",
        "    desccription=[]\n",
        "    #page_contents=response.text\n",
        "    soup = BeautifulSoup(page_contents, 'html.parser')  \n",
        "    desc_selection_class='f5 color-fg-muted mb-0 mt-1'\n",
        "    desc_tags=soup.find_all('p',{'class': desc_selection_class})\n",
        "    for tag in desc_tags:\n",
        "      desccription.append(tag.text.strip())\n",
        "      len(desccription)\n",
        "  except:\n",
        "    raise Exception('failed to extract the topics')\n",
        "  return desccription\n",
        "  "
      ],
      "metadata": {
        "id": "zA5ok5M1kppo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_disc=fn_extract_the_description()"
      ],
      "metadata": {
        "id": "WNt54JtiorqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fn_extract_the_topics_link() will extract the URL links for the github"
      ],
      "metadata": {
        "id": "SLNZAiWJbp5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fn_extract_the_topics_link():\n",
        "  \n",
        "  base_url=\"https://github.com\"\n",
        "  try:\n",
        "    url_links=[]\n",
        "    #page_contents=response.text\n",
        "    soup = BeautifulSoup(page_contents, 'html.parser')\n",
        "    slect_link_tags='d-flex no-underline'\n",
        "    topic_link_tags=soup.find_all('a',{'class': slect_link_tags})\n",
        "    for link in topic_link_tags:\n",
        "      url_links.append(base_url+link['href'])\n",
        "      #print(url_links)\n",
        "  except:\n",
        "      raise Exception('failed to extract the topics')\n",
        "  return url_links\n",
        "  \n"
      ],
      "metadata": {
        "id": "1tKEPkk9tRbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_link=fn_extract_the_topics_link()"
      ],
      "metadata": {
        "id": "azsMJbuwuSOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are creating the data frame."
      ],
      "metadata": {
        "id": "b42TBRlMbtgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_frame():\n",
        "  topics_dict={\n",
        "    'skills':skill_top,\n",
        "    'skill decription':top_disc,\n",
        "    'skill link': top_link\n",
        "    }\n",
        "  try:\n",
        "    \n",
        "    topic_df = pd.DataFrame(topics_dict)\n",
        "    print(topic_df)\n",
        "    \n",
        "  except:\n",
        "     print(\"Error\") \n",
        "  return topic_df\n"
      ],
      "metadata": {
        "id": "axpjtVuRlWF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_df=create_data_frame()"
      ],
      "metadata": {
        "id": "pL8obCM0pkbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save the data in the csv file."
      ],
      "metadata": {
        "id": "y2pS0Tdmbx8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fn_save_datatocsv():\n",
        "  try:\n",
        "    topic_df.to_csv('github hot topic.csv',index=None)\n",
        "  except:\n",
        "    raise Exception('failed to extract the topics')\n"
      ],
      "metadata": {
        "id": "WAG0U8q8zOav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fn_save_datatocsv()"
      ],
      "metadata": {
        "id": "UOX6f2gvEJbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting topic information from the topic page"
      ],
      "metadata": {
        "id": "bB8JERKvcwGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_the_topic_url(link):\n",
        "  #print(link)\n",
        "  topic_page_url =link\n",
        "  print(topic_page_url)\n",
        "  return topic_page_url"
      ],
      "metadata": {
        "id": "6AcMVqXwc244"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topic_info(topic_url):\n",
        "  try:\n",
        "    #topic_page_url =topic_url[0\n",
        "    #topic_page_url\n",
        "    \n",
        "    print(topic_url)    \n",
        "    response=requests.get(topic_url)\n",
        "    response.status_code\n",
        "    len(response.text)\n",
        "    sills_page_contents =response.text\n",
        "    #print(sills_page_contents)\n",
        "  except:\n",
        "    raise Exception \n",
        "  return sills_page_contents   \n"
      ],
      "metadata": {
        "id": "ivuP7tX2k67u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LrWO9q-uBYWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_page_content(h1_tag,doc_content):\n",
        "    soup = BeautifulSoup(doc_content, 'html.parser')\n",
        "    username_class='f3 color-fg-muted text-normal lh-condensed'\n",
        "    repos_tags=soup.find_all('h3',{'class': username_class})\n",
        "    a_tags=h1_tag.find_all('a')\n",
        "    return soup,repos_tags"
      ],
      "metadata": {
        "id": "b3tCk-KosAY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_tags=repos_tags.find_all('a')\n",
        "baseUrl='https://github.com/'\n",
        "star_class='Counter js-social-count'\n",
        "soup = BeautifulSoup(doc_content, 'html.parser')\n",
        "username_class='f3 color-fg-muted text-normal lh-condensed'\n",
        "repos_tags=soup.find_all('h3',{'class': username_class})\n",
        "#print(repos_tags[0])\n",
        "#print(h1_tag)\n",
        "a_tags=repos_tags.find_all('a')\n",
        "#print(a_tags)\n",
        "username=a_tags[0].text.strip()\n",
        "#print(username)\n",
        "repo_name= a_tags[1].text.strip()\n",
        "#print(repo_name)\n",
        "repo_url=baseUrl+a_tags[1]['href']\n",
        "#print(repo_url)\n",
        "repo_stars=soup.find_all('span',{'class': star_class})\n",
        "star_count=repo_stars[0]['title']\n",
        "star_count.strip()\n",
        "print(star_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "Mm5f-4LdAEc7",
        "outputId": "9f6e36ea-9353-4872-a3a6-4f264822a56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-ce9b07cfeed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepos_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbaseUrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://github.com/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstar_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Counter js-social-count'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0musername_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f3 color-fg-muted text-normal lh-condensed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2252\u001b[0m         \u001b[0;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m         raise AttributeError(\n\u001b[0;32m-> 2254\u001b[0;31m             \u001b[0;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2255\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url4=top_link[8]\n",
        "url4\n",
        "doc=topic_info(url4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liefNtcvAoXV",
        "outputId": "f6906c61-45ac-4fd0-9d84-3691bcb4a400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://github.com/topics/arduino\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(doc, 'html.parser')\n",
        "username_class='f3 color-fg-muted text-normal lh-condensed'\n",
        "repos_tags=soup.find_all('h3',{'class': username_class})"
      ],
      "metadata": {
        "id": "2t6Vs7n6AZrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_repo_info(h1_tag,doc_content):\n",
        "  #print(h1_tag)\n",
        "  a_tags=h1_tag.find_all('a')\n",
        "  baseUrl='https://github.com/'\n",
        "  star_class='Counter js-social-count'\n",
        "  soup = BeautifulSoup(doc_content, 'html.parser')\n",
        "  username_class='f3 color-fg-muted text-normal lh-condensed'\n",
        "  repos_tags=soup.find_all('h3',{'class': username_class})\n",
        "  #print(repos_tags[0])\n",
        "  #print(h1_tag)\n",
        "  a_tags=h1_tag.find_all('a')\n",
        "  #print(a_tags)\n",
        "  username=a_tags[0].text.strip()\n",
        "  #print(username)\n",
        "  repo_name= a_tags[1].text.strip()\n",
        "  #print(repo_name)\n",
        "  repo_url=baseUrl+a_tags[1]['href']\n",
        "  #print(repo_url)\n",
        "  repo_stars=soup.find_all('span',{'class': star_class})\n",
        "  star_count=repo_stars[0]['title']\n",
        "  star_count.strip()\n",
        "  print(star_count)\n",
        "  return username,repo_name,repo_url,star_count\n"
      ],
      "metadata": {
        "id": "bcw2TdOkajon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_All_data_in_dictionary(reprepos_tags):\n",
        "  topic_respos_dic={\n",
        "  'username':[],\n",
        "  'repo_name':[],\n",
        "  'star':[],\n",
        "  'rep_url':[]\n",
        "  }\n",
        " \n",
        "  for i in range(len(reprepos_tags)):\n",
        "    print(len(reprepos_tags))\n",
        "    repo=get_repo_info(reprepos_tags[i])\n",
        "    print(repo)\n",
        "  #print(repo[1])\n",
        "    topic_respos_dic['username'].append(repo[0])\n",
        "    print(repo[0])\n",
        "    topic_respos_dic['repo_name'].append(repo[1])\n",
        "    topic_respos_dic['rep_url'].append(repo[2])\n",
        "    topic_respos_dic['star'].append(repo[3])\n",
        "    print(topic_respos_dic)    \n",
        "  return topic_respos_dic\n"
      ],
      "metadata": {
        "id": "qTod0W3qzHaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CB5HkaEGJ_mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fn_create_topic_df(topic_respos_dic):\n",
        "  topic_repos_df=pd.DataFrame(topic_respos_dic)\n",
        "  print(topic_repos_df)"
      ],
      "metadata": {
        "id": "q_NHq5mj4Veh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url4=top_link[8]\n",
        "url4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PxwI6q7KELtQ",
        "outputId": "a411b89c-4519-4854-d1ee-ff06d8be2b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://github.com/topics/arduino'"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "aRkbuGQPEaWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link=get_the_topic_url(url4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb4VeMCrF_ev",
        "outputId": "e67f26c5-aeb0-4eb6-f51a-105458de84e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://github.com/topics/arduino\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc=topic_info(link)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IPhTVPj1nk1",
        "outputId": "015c4d45-4c00-4805-f04c-989aa462afe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://github.com/topics/arduino\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repos_tags=read_page_content(repos_tags,doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "p3SRwT8M11x5",
        "outputId": "eda188f7-51c1-4bb8-dbe2-0290d1f8878b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-a34b312c1b34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrepos_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_page_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepos_tags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-675dc5c01a0e>\u001b[0m in \u001b[0;36mread_page_content\u001b[0;34m(h1_tag, doc_content)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0musername_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f3 color-fg-muted text-normal lh-condensed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrepos_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0musername_class\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0ma_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh1_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepos_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'find_all'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_repo_info(repos_tags,doc)"
      ],
      "metadata": {
        "id": "R0aTKB9F2YLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c0Pmil1020Vb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}